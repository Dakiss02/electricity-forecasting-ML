# -*- coding: utf-8 -*-
"""Electricity Forecast ML - DCMME

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qlJ7PdSLuJ4M6ezO-fTorOhbjf7swCXA

#CRISP-DM Methodology

*Library Installation*
"""

# ==========================================
# ðŸ“š LIBRARIES IMPORTATION
# ==========================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Stats and ML
import statsmodels.api as sm
from scipy.stats import skew, shapiro
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.stattools import durbin_watson
from statsmodels.stats.diagnostic import het_breuschpagan
from sklearn.linear_model import LinearRegression, RidgeCV
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    mean_squared_error, r2_score, mean_absolute_error,
    accuracy_score, f1_score, roc_auc_score
)
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance

# 3D plotting (optional)
from mpl_toolkits.mplot3d import Axes3D

# Suppress warnings
warnings.filterwarnings('ignore')

print("âœ… Properly imported libraries.")

print("\nSetting up access to Google Drive...")
try:
    from google.colab import drive
    drive.mount('/content/drive')
    print("Google Drive set up correctly")
    file_path = '/content/drive/MyDrive/2025-1/Monografia/Datasets & Data Dictionary/Current! Monthly Datasets and Data Dictionary/df copy.csv'
    print(f"Reading field...")
    df = pd.read_csv(file_path)
    print("Dataset loaded successfully!")
    print(f"ðŸ“Š Dataset dimensions: {df.shape}")
    print(f"ðŸ“‹ Columns: {list(df.columns)}")
except FileNotFoundError:
    print(f"It was not found specified route")
    print("Verify route ")
except Exception as e:
    print(f"Error uploading dataset: {e}")

"""# Data Overview"""

# Columns and rows
df.dropna(how='all').shape
print(df.tail())

"""**Variable Types:** consider datetime, floats and int variables:"""

df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y', errors='coerce')  # coerce transform errors to NaT

# float o int
float_cols = [
    'fossil fuels', 'renewable energy', 'mixed energy', 'others',
    'real_gdp', 'policy_uncertainty_index',
    'electricity_sales_residential', 'electricity_sales_commercial',
    'electricity_sales_industrial', 'electricity_sales_transportation',
    'electricity_end_use',
]

int_cols = [
    'population', 'heating_degree-days', 'cooling_degree-days',
    'bev', 'phev', 'hev', 'total_ldv', 'total_ev_stock'
]

# Apply conversions
df[float_cols] = df[float_cols].apply(pd.to_numeric, errors='coerce')

for col in int_cols:
    if col in df.columns:
        try:
            # Convertir a numÃ©rico primero, los errores se convierten a NaN
            df[col] = pd.to_numeric(df[col], errors='coerce')
            # Luego convertir a Int64. Esto manejarÃ¡ los NaN introducidos por coerce
            df[col] = df[col].astype('Int64')
        except Exception as e:
            print(f"Error converting column '{col}' to Int64: {e}")
    else:
        print(f"Column '{col}' not found in DataFrame.")


# Verificcation
print(df.dtypes)
print(df.tail())

"""**Variables**

"""

#All variables
all_variables = [
    'fossil fuels', 'mixed energy', 'renewable energy', 'real_gdp',
    'policy_uncertainty_index', 'heating_degree-days', 'cooling_degree-days',
    'total_ev_stock', 'electricity_end_use', 'electricity_sales_residential',
    'electricity_sales_commercial', 'electricity_sales_industrial', 'electricity_sales_transportation'
]
# Independient variables (without date)
independent_variables = [
    'fossil fuels', 'mixed energy', 'renewable energy', 'real_gdp',
    'policy_uncertainty_index', 'heating_degree-days', 'cooling_degree-days',
    'total_ev_stock', 'electricity_sales_residential', 'electricity_sales_commercial',
    'electricity_sales_industrial', 'electricity_sales_transportation'
]
# Dependent Variable
dependent_variable = 'electricity_end_use'

"""#3. Data Preparation
The purpose of Data Preparation phase is transform and modify the variables as much as they need to increase the accuracy and factibility of the forecasting model.

**Data Understanding conclusions**

* There are not null values, what is ideal in modeling.
* Good definition of data type, facilitating statistical and visual analysis.

"""

# Complete list of needed variables
all_variables = independent_variables + [dependent_variable]

# New DataFrame base
df_base = df[all_variables + ['date']].copy()

# Lag t-1 to all independent variables
for var in independent_variables:
    df_base[var] = df_base[var].shift(1)
df_base.dropna(inplace=True) #eliminar nulls

# Verify structure and nulls
print(df_base.info())
print()
print(df_base.isnull().sum())

"""**Seasons variable**"""

def get_season(month):
    if month in [12, 1, 2]:
        return 'winter'
    elif month in [3, 4, 5]:
        return 'spring'
    elif month in [6, 7, 8]:
        return 'summer'
    else:
        return 'fall'

# New variable "Seasons"- Category
df_base['season'] = df_base['date'].dt.month.map(get_season)

# Verification
print(df_base[['date', 'season']].head())

# Create Dummy variables
df_base = pd.get_dummies(df_base, columns=['season'], drop_first=True)
print(df_base.tail())

# Identify bool columns
bool_cols = df_base.select_dtypes(include='bool').columns

# Conversion to Int (0 and 1)
df_base[bool_cols] = df_base[bool_cols].astype(int)

df_base.to_csv('df_base .csv', index=False)

print(df['electricity_end_use'].mean())

"""#4. Modeling

**Functional Non-Transformation**

To preserve the nature of the variables, the transformations will be applied based on the correlogram results. Specifically: for variables showing a trend, differencing will be used; for variables exhibiting cyclicality, moving averages will be applied. These techniques help smooth the behavior of the variables, sacrificing some observations in order to allow them to operate based on their own prior values and get stationaries variables.

*Differentiation:* It consist in resting the value immediately before to the actual.

*Mobile Averages:*
"""

df_trans = df_base.copy()
#df_trans.to_csv("antestrans.csv", index=False)

diff_variables = ['renewable energy', 'real_gdp', 'policy_uncertainty_index', 'total_ev_stock', 'electricity_sales_transportation']
med3_variables = ['fossil fuels', 'mixed energy', 'electricity_sales_residential']
med6_variables = ['heating_degree-days', 'cooling_degree-days','electricity_sales_commercial', 'electricity_sales_industrial']

for var in diff_variables:
    df_trans[f'{var}_diff'] = df_trans[var].diff()

for var in med3_variables:
    df_trans[f'{var}_ma3'] = df_trans[var].rolling(window=3).mean()

for var in med6_variables:
    df_trans[f'{var}_ma6'] = df_trans[var].rolling(window=6).mean()

# To delete original variables in case there are new to replace them
df_trans.drop(columns=[col for col in independent_variables if col in df_trans.columns], inplace=True)

df_trans.dropna(inplace=True)
df_trans.columns
df_trans.tail()

#df_trans.to_csv("outputf.csv", index=False)
print(df_trans.dtypes)

inddf_trans = df_trans[['date','season_spring', 'season_summer', 'season_winter',
       'renewable energy_diff', 'real_gdp_diff',
       'policy_uncertainty_index_diff', 'total_ev_stock_diff',
       'electricity_sales_transportation_diff', 'fossil fuels_ma3',
       'mixed energy_ma3', 'electricity_sales_residential_ma3', 'heating_degree-days_ma6',
       'cooling_degree-days_ma6', 'electricity_sales_commercial_ma6',
       'electricity_sales_industrial_ma6']]
depdf_trans = df_trans[['electricity_end_use']]

df_transm = pd.concat([inddf_trans, depdf_trans], axis=1)
df_transm.set_index('date', inplace=True)
print(df_transm.dtypes)

"""**Z normalization**"""

from sklearn.preprocessing import StandardScaler
# Seleccionar columnas numÃ©ricas que quieras estandarizar
cols_to_scale = df_trans.select_dtypes(include=[np.number]).columns
cols_to_scale = cols_to_scale.drop('electricity_end_use')

# Aplicar Z-score
scaler = StandardScaler()
df_scaled = df_trans.copy()
df_scaled[cols_to_scale] = scaler.fit_transform(df_trans[cols_to_scale])

inddf_scd = df_scaled[['date','season_spring', 'season_summer', 'season_winter',
       'renewable energy_diff', 'real_gdp_diff',
       'policy_uncertainty_index_diff', 'total_ev_stock_diff',
       'electricity_sales_transportation_diff', 'fossil fuels_ma3',
       'mixed energy_ma3', 'electricity_sales_residential_ma3', 'heating_degree-days_ma6',
       'cooling_degree-days_ma6', 'electricity_sales_commercial_ma6',
       'electricity_sales_industrial_ma6']]
depdf_scd = df_scaled[['electricity_end_use']]

df_scaledm = pd.concat([inddf_scd, depdf_scd], axis=1)
df_scaledm.set_index('date', inplace=True)

df_train2 = df_scaledm.loc[:'2022-12-31'].dropna()
df_test2 = df_scaledm.loc['2023-01-01':]

df_scaledm.to_csv('df_2.csv', index=False)

"""**Ridge**"""

# Variables selection - Based on iteration 8
ind_vars_list12 = [
    'season_summer',
    'season_spring',
    'electricity_sales_residential_ma3',
    'cooling_degree-days_ma6',
    'fossil fuels_ma3',
    'season_winter',
    'total_ev_stock_diff',
    'policy_uncertainty_index_diff'
]

dep_var_list12 = ['electricity_end_use']

x12 = df_train2[ind_vars_list12]
y12 = df_train2[dep_var_list12]

print("Variables independientes:", x12.columns.tolist())
print("Variable dependiente:", dep_var_list12)

alphas = np.logspace(-3, 3, 50)

ridge_cv12 = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv12.fit(x12, y12)

print("Alpha Ã³ptimo:", ridge_cv12.alpha_)
print("RÂ² (train):", ridge_cv12.score(x12, y12))
y12_pred = ridge_cv12.predict(x12)
rmse12 = np.sqrt(mean_squared_error(y12, y12_pred))
print("RMSE (train):", rmse12)

result12 = permutation_importance(ridge_cv12, x12, y12, n_repeats=30, random_state=0)
importance12 = pd.Series(result12.importances_mean, index=x12.columns).sort_values(ascending=False)
importance_rel12 = (importance12 / importance12.sum()) * 100
print(importance_rel12)

df_train2.to_csv('dftrain2.csv', index=False)

#Download
df_train_results = pd.DataFrame({
    'electricity_end_use_actual': y12.values.flatten(),
    'electricity_end_use_predicted': y12_pred.flatten()
}, index=x12.index)

df_train_results.to_csv('ridge_iteration12_train.csv', index_label='datetime')
print("Archivo ridge_iteration12_train.csv guardado.")

"""# Evaluation"""

# Test
x_test12 = df_test2[ind_vars_list12]
y_test12 = df_test2[dep_var_list12]

# Predictions
y_pred_test12 = ridge_cv12.predict(x_test12)

from sklearn.metrics import mean_squared_error, r2_score

rmse_test12 = np.sqrt(mean_squared_error(y_test12, y_pred_test12))
r2_test12 = r2_score(y_test12, y_pred_test12)

# Results
print("ðŸ“˜  Evaluation in testing (Model 2):")
print("RÂ² (test):", r2_test12)
print("RMSE (test):", rmse_test12)

x_test12

y_test12

#Normality
import scipy.stats as stats
import seaborn as sns
import statsmodels.api as sm

# Residues
y12_series = y12.squeeze()
residuals12 = y12_series - y12_pred

# Shapiro-Wilk test
shapiro_test = stats.shapiro(residuals12)
print("Shapiro-Wilk test:")
print("W =", shapiro_test.statistic, "| p-value =", shapiro_test.pvalue)

# Multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

x12_numeric = x12.select_dtypes(include=["number"]).astype(float)
x12_numeric = x12_numeric.replace([np.inf, -np.inf], np.nan)
x12_numeric = x12_numeric.dropna()

# Calculate VIF
vif12_data = pd.DataFrame()
vif12_data["Variable"] = x12_numeric.columns
vif12_data["VIF"] = [variance_inflation_factor(x12_numeric.values, i)
                    for i in range(x12_numeric.shape[1])]

print(vif12_data)

# Autocorrelation of residues
from statsmodels.stats.stattools import durbin_watson

dw_stat = durbin_watson(residuals12)
print("Durbin-Watson:", dw_stat)

#Homocedasticity
import statsmodels.api as sm

x12_bp = sm.add_constant(x12)
bp_test = sm.stats.diagnostic.het_breuschpagan(residuals12, x12_bp)

labels = ['LM Stat', 'LM p-val', 'F Stat', 'F p-val']
print(dict(zip(labels, bp_test)))

# Condition Index Test
from numpy.linalg import svd

# 1. Matrix X without scaling
X12 = x12.astype(float).values

# 2. Compute SVD of X
U, s, Vt = svd(X12, full_matrices=False)

# 3. Compute eigenvalues
eigenvalues = s**2 / (X12.shape[0] - 1)

# 4. Compute Condition Index
condition_index = np.sqrt(eigenvalues.max() / eigenvalues)

# 5. Display results
condition_df = pd.DataFrame({
    "Eigenvalue": eigenvalues,
    "Condition Index": condition_index
})

print("\nCondition Index by dimension:")
print(condition_df)

# Or just the maximum index:
max_condition_index = condition_index.max()
print(f"\nðŸ”Ž Maximum Condition Index: {max_condition_index}")
print(x12.dtypes)

#Download
df_test_results = pd.DataFrame({
    'electricity_end_use_actual': y_test12.values.flatten(),
    'electricity_end_use_predicted': y_pred_test12.flatten()
}, index=x_test12.index)

# CVS
df_test_results.to_csv('ridge_iteration12_test.csv', index_label='datetime')
print("Archivo ridge_iteration12_test.csv guardado.")

"""# Deployment

**Independent variables settings**
"""

df_test2_reset = df_test2.reset_index()
print(df_test2_reset.columns)

scaler12 = StandardScaler()
scaler12.fit(x12)

#Future Calendar

start_date = '2025-02-01'
periods = 60

future_dates = pd.date_range(start=start_date, periods=periods, freq='MS')

def get_season(month):
    if month in [12, 1, 2]:
        return 'winter'
    elif month in [3, 4, 5]:
        return 'spring'
    elif month in [6, 7, 8]:
        return 'summer'
    else:
        return 'fall'

df_future = pd.DataFrame({'date': future_dates})
df_future['season'] = df_future['date'].dt.month.map(get_season)

# Crear dummies
df_future = pd.get_dummies(df_future, columns=['season'], drop_first=True)

"""**Forecasting**"""

n_periods = 60
freq = "MS"
feature_ord = ridge_cv12.feature_names_in_
last_date = x_test12.index[-1]
idx_future = pd.date_range(last_date + pd.offsets.MonthBegin(),
                          periods=n_periods, freq=freq)

print(f"Predicting from: {idx_future[0].strftime('%Y-%m-%d')}")
print(f"Until: {idx_future[-1].strftime('%Y-%m-%d')}")
print(f"Total periods: {n_periods}")
print()

# Initialize DataFrame
X_future = pd.DataFrame(index=idx_future, columns=feature_ord, dtype=float)
hist = x_test12.copy()
if not isinstance(hist.index, pd.DatetimeIndex):
    hist.index = pd.to_datetime(hist.index)
hist = hist.copy()
hist["month"] = hist.index.month
hist["year"] = hist.index.year

seasonal_keywords = ['season', 'cooling', 'heating', 'degree', 'summer', 'winter', 'spring']
seasonal_vars = [col for col in feature_ord if any(keyword in col.lower() for keyword in seasonal_keywords)]

print(f"Processing {len(seasonal_vars)} seasonal variables")
for col in seasonal_vars:
    if col in hist.columns:
        try:
            monthly_avg = hist.groupby("month")[col].mean()
            yearly_avg = hist.groupby("year")[col].mean()

            if len(yearly_avg) > 1:
                years = yearly_avg.index.values
                values = yearly_avg.values
                trend_slope = np.polyfit(years, values, 1)[0]
                future_years = idx_future.year.values
                trend_adjustment = trend_slope * (future_years - years[-1])

                base_values = idx_future.month.map(monthly_avg).values
                X_future[col] = base_values + trend_adjustment
                print(f"{col}: trend applied (slope={trend_slope:.4f})")
            else:
                X_future[col] = idx_future.month.map(monthly_avg).values
                print(f"    {col}: only monthly average (few years)")
        except Exception as e:
            print(f"Error processing {col}: {e}")
            if col in hist.columns:
                monthly_avg = hist.groupby("month")[col].mean()
                X_future[col] = idx_future.month.map(monthly_avg).fillna(hist[col].mean()).values
            else:
                X_future[col] = 0.0
    else:
        X_future[col] = 0.0
        print(f" {col}: variable not found, using 0.0")

price_keywords = ['price', 'fuel', 'sales', 'stock', 'electricity']
price_vars = [col for col in feature_ord if any(keyword in col.lower() for keyword in price_keywords)]

print(f"Processing {len(price_vars)} economic variables")
for col in price_vars:
    if col in hist.columns:
        try:
            X_trend = np.arange(len(hist)).reshape(-1, 1)
            y_trend = hist[col].values
            mask = ~np.isnan(y_trend)
            if mask.sum() > 2:
                trend_model = LinearRegression()
                trend_model.fit(X_trend[mask], y_trend[mask])
                future_X = np.arange(len(hist), len(hist) + n_periods).reshape(-1, 1)
                future_values = trend_model.predict(future_X)
                X_future[col] = future_values
                print(f"  {col}: linear extrapolation applied")
            else:
                monthly_avg = hist.groupby("month")[col].mean()
                X_future[col] = idx_future.month.map(monthly_avg).fillna(hist[col].mean()).values
                print(f"{col}: monthly average used (few valid data points)")
        except Exception as e:
            print(f"Error processing {col}: {e}")
            # Basic fallback
            X_future[col] = hist[col].mean() if col in hist.columns else 0.0
    else:
        X_future[col] = 0.0
        print(f" {col}: variable not found, using 0.0")

diff_vars = [col for col in feature_ord if 'diff' in col.lower()]
policy_vars = [col for col in feature_ord if 'policy' in col.lower() or 'uncertainty' in col.lower()]

print(f"Processing {len(diff_vars)} differenced variables")
for col in diff_vars:
    if col in hist.columns:
        recent_values = hist[col].dropna().tail(6)
        if len(recent_values) > 0:
            mean_diff = recent_values.mean()
            std_diff = recent_values.std()
            ar_values = []
            last_val = recent_values.iloc[-1]

            for i in range(n_periods):
                phi = 0.3
                next_val = (phi * last_val +
                           (1-phi) * mean_diff +
                           np.random.normal(0, std_diff * 0.3))
                ar_values.append(next_val)
                last_val = next_val

            X_future[col] = ar_values
        else:
            X_future[col] = 0.0
    else:
        X_future[col] = 0.0


print(f"Processing {len(policy_vars)} policy variables")
for col in policy_vars:
    if col in hist.columns:
        weights = np.exp(0.1 * (hist.index.year - hist.index.year.min()))
        weighted_mean = np.average(hist[col].dropna(), weights=weights[hist[col].notna()])
        monthly_avg = hist.groupby("month")[col].mean()
        seasonal_component = idx_future.month.map(monthly_avg).fillna(weighted_mean)

        X_future[col] = seasonal_component.values
    else:
        X_future[col] = 0.0

processed_vars = set(seasonal_vars + price_vars + diff_vars + policy_vars)
remaining_vars = [col for col in feature_ord if col not in processed_vars]
for col in remaining_vars:
    if col in hist.columns:
        monthly_avg = hist.groupby("month")[col].mean()
        X_future[col] = idx_future.month.map(monthly_avg).values
    else:
        X_future[col] = 0.0
for col in X_future.columns:
    if col in hist.columns:
        hist_mean = hist[col].mean()
        hist_std = hist[col].std()
        X_future[col] = np.clip(X_future[col],
                               hist_mean - 3*hist_std,
                               hist_mean + 3*hist_std)

print("X_future matrix ready")

y_future = ridge_cv12.predict(X_future).ravel()

pred_df = pd.DataFrame(
    {
        "electricity_end_use_pred": y_future
    },
    index=idx_future
)

pred_df.index.name = "date"

pred_df

"""**Feedback model results**"""

import matplotlib.pyplot as plt

# --------------------------------------------
# REAL
# --------------------------------------------

# Real - TRAIN
real_train = df_train_results['electricity_end_use_actual']
# Predicted - TRAIN
pred_train = df_train_results['electricity_end_use_predicted']

# Real - TEST
real_test = df_test_results['electricity_end_use_actual']
# Predicted - TEST
pred_test = df_test_results['electricity_end_use_predicted']

# Real - FORECAST (only predicted, no actual values)
pred_forecast = pred_df['electricity_end_use_pred']

# --------------------------------------------
# Plot
# --------------------------------------------

plt.figure(figsize=(14,6))

# ------------------------
# Solid REAL line
# ------------------------
# Train
plt.plot(real_train.index, real_train.values,
         color='blue', linestyle='-', linewidth=2, label='Actual (Train)')

# Test
plt.plot(real_test.index, real_test.values,
         color='green', linestyle='-', linewidth=2, label='Actual (Test)')

# Forecast has no real line. If it existed, it would be plotted here.

# ------------------------
# Dashed PREDICTED lines
# ------------------------
# Train
plt.plot(pred_train.index, pred_train.values,
         color='blue', linestyle='--', linewidth=2, label='Model (Train)')

# Test
plt.plot(pred_test.index, pred_test.values,
         color='green', linestyle='--', linewidth=2, label='Model (Test)')

# Forecast
plt.plot(pred_df.index, pred_forecast.values,
         color='red', linestyle='--', linewidth=2, label='Forecast')

plt.title('Actual Electricity vs. Predictions & Final Forecast')
plt.xlabel('Date')
plt.ylabel('Electricity (MWh)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()