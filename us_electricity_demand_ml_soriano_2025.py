# -*- coding: utf-8 -*-
"""US_Electricity_Demand_ML_Soriano_2025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wxK6Rg5zBvWUdkYtVte4RfWhQLC-ZCOz

#CRISP-DM Methodology

###*Library Installation*
"""

# ==========================================
# 📚 LIBRARIES IMPORTATION
# ==========================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Stats and ML
import statsmodels.api as sm
from scipy.stats import skew, shapiro
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.stattools import durbin_watson
from statsmodels.stats.diagnostic import het_breuschpagan
from sklearn.linear_model import LinearRegression, RidgeCV
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    mean_squared_error, r2_score, mean_absolute_error,
    accuracy_score, f1_score, roc_auc_score
)
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance

# 3D plotting (optional)
from mpl_toolkits.mplot3d import Axes3D

# Suppress warnings
warnings.filterwarnings('ignore')

print("✅ Properly imported libraries.")

print("\nSetting up access to Google Drive...")
try:
    from google.colab import drive
    drive.mount('/content/drive')
    print("Google Drive set up correctly")
    file_path = '/content/drive/MyDrive/2025-1/Monografia/Datasets & Data Dictionary/Current! Monthly Datasets and Data Dictionary/df copy.csv'
    print(f"Reading field...")
    df = pd.read_csv(file_path)
    print("Dataset loaded successfully!")
    print(f"📊 Dataset dimensions: {df.shape}")
    print(f"📋 Columns: {list(df.columns)}")
except FileNotFoundError:
    print(f"It was not found specified route")
    print("Verify route ")
except Exception as e:
    print(f"Error uploading dataset: {e}")

"""# Data Understanding

This section of the CRISP-DM methodology, connects initial data, describes the content and explores the quality and possibilities regarding the Business Understanding and the main objetive of the project. It implies the understanding of quality, temporality, relevance and parametters regarded each variable. In order to obtain it, the phase is defined in:

* Data cleaning and Overview
* Structural Analysis
* Environmental context

###  Initial Data Quality Check

**Manual Extraction**

- Temperature variables were already available in CSV format with the correct structure.
- Real GDP data required manual editing in spreadsheets due to its size and formatting.
- Policy uncertainty data involved removing two years of observations.
- EV sales data also required spreadsheet adjustments in terms of quantity and infrastructure representation.


**Programmatic Extraction (Python):**
- Electricity mix required a complex data extraction process due to its structure and classification.
"""

# Columns and rows
df.dropna(how='all').shape
print(df.tail())

"""**Variable Types:** consider datetime, floats and int variables:"""

df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y', errors='coerce')  # coerce transform errors to NaT

# float o int
float_cols = [
    'fossil fuels', 'renewable energy', 'mixed energy', 'others',
    'real_gdp', 'policy_uncertainty_index',
    'electricity_sales_residential', 'electricity_sales_commercial',
    'electricity_sales_industrial', 'electricity_sales_transportation',
    'electricity_end_use',
]

int_cols = [
    'population', 'heating_degree-days', 'cooling_degree-days',
    'bev', 'phev', 'hev', 'total_ldv', 'total_ev_stock'
]

# Apply conversions
df[float_cols] = df[float_cols].apply(pd.to_numeric, errors='coerce')

for col in int_cols:
    if col in df.columns:
        try:
            # Convertir a numérico primero, los errores se convierten a NaN
            df[col] = pd.to_numeric(df[col], errors='coerce')
            # Luego convertir a Int64. Esto manejará los NaN introducidos por coerce
            df[col] = df[col].astype('Int64')
        except Exception as e:
            print(f"Error converting column '{col}' to Int64: {e}")
    else:
        print(f"Column '{col}' not found in DataFrame.")


# Verificcation
print(df.dtypes)
print(df.tail())

numeric_df = df.select_dtypes(include=[np.number])# Selection just numeric columns to calculate matrix

correlation_matrix = numeric_df.corr()
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool)) # Superior mask

plt.figure(figsize=(16, 12))
sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt=".2f", cmap='coolwarm', square=True, linewidths=.5, cbar_kws={"shrink": .8})
plt.title("Correlation Matrix between Numeric Variables")
plt.tight_layout()
plt.show()

"""**Observations**

The following list explains some of the correlations:

* *Too Strong positive Correlations (multicolineality problem)*
  * Population and Real GDP: Their contribution is too similar.
  * BEV, PHEV, HEV, EV Stock and EV Total Stock: The ev_stock_weighted contains all needed and avoid multicolinearity
**Strong Positive Correlations with important Variable*
  * Renewable Energy with: Real GDP and policy uncertainty
  * EV Stock with Real GDP and Renewable energy

* *Strong Positive Correlations with the dependant variable*
  * Fossil fuels, Mixed energy, others and Electricity Sales (by sector)


Due to the above, some variables will be deleted completely of the model:

**Variables**

| **Variables**                      | **Data Type** | **Relevance** | **Units of Measurement**           |
|----------------------------------|---------------|---------------------|------------------------------------|
| date                             | object        | High                   | YYYYMMDD                           |
| fossil fuels                     | float64       | High                   | MWh                                |
| mixed energy                     | float64       | High                   | MWh                                |                                |
| renewable energy                 | float64       | Moderate                    | MWh                                |
| real_gdp                         | float64       | Moderate                   | SAAR billions of US dollars        |
| policy_uncertainty_index        | float64       | Moderate                   | NSA Index                          |
| heating_degree-days              | int64         | Moderate                   | F°                                 |
| cooling_degree-days              | int64         | High                   | F°                                 |
| total_ev_stock                   | int64         | High                   | # vehicles                         |
| electricity_sales_residential    | float64       | High                   | MWh                        |
| electricity_sales_commercial     | float64       | High                   | MWh                        |
| electricity_sales_industrial     | float64       | High                   | MWh                        |                       |
| electricity_sales_transportation              | float64       | Moderate                   | MWh
| electricity_end_use              | float64       | Highest                   | MWh  

***Moderate Variables to keep even with lower relations.***
* policy_uncertainty_index: moderate relation. Useful for environmental context.
* heating_degree_days: temperature importance.
* renewable_energy: energetic mix analysis.
* real_gdp: Economic considerations.
* ev_stock_weighted: relevant variable for the study.

***Variables out:***
* Others: without theorical relevancy.
* Population: too similar to Real GDP.
* Total LDV, PHEV, HEV, BEV and total EV Stock.
"""

#All variables
all_variables = [
    'fossil fuels', 'mixed energy', 'renewable energy', 'real_gdp',
    'policy_uncertainty_index', 'heating_degree-days', 'cooling_degree-days',
    'total_ev_stock', 'electricity_end_use', 'electricity_sales_residential',
    'electricity_sales_commercial', 'electricity_sales_industrial', 'electricity_sales_transportation'
]
# Independient variables (without date)
independent_variables = [
    'fossil fuels', 'mixed energy', 'renewable energy', 'real_gdp',
    'policy_uncertainty_index', 'heating_degree-days', 'cooling_degree-days',
    'total_ev_stock', 'electricity_sales_residential', 'electricity_sales_commercial',
    'electricity_sales_industrial', 'electricity_sales_transportation'
]
# Dependent Variable
dependent_variable = 'electricity_end_use'

"""###Data Overview

* Variables: 12.
* Categories of variables:  economic, climatic, policy and electric vehicles variables.
* Datasets: 8 sources (Energy Information Aadministration (EIA), Macrotrends, Bureau of Transportation Statistics (BTS), the Federal Reserve Bank, among others.).
* Frequency: Monthly.
* Agregation level: National, U.S.

## Exploratory Data Analysis

To obtain a deeper understanding on the data, this analysis contains:

1. Correlation matrix: to delete the noisy variables based on the relation between all of them.
2. Histograms: To identify the simmetry, anormality and atypical values distribution and shape of the variables. This is followed by skewness and kurtosis.
3. Scatter Plots: To understand the normality of the variables and find outliers.
4. Correlograms: It helps to find the state of stationality in the variables over the time.

### Univariate Analysis

#### Histograms
"""

num_vars = len(all_variables)
fig, axes = plt.subplots(nrows=(num_vars + 2) // 3, ncols=3, figsize=(18, 4 * ((num_vars + 2) // 3)))
axes = axes.flatten()

for i, var in enumerate(all_variables):
    if var in df.columns:
        sns.histplot(df[var].dropna(), kde=True, ax=axes[i], bins=30, color='skyblue')
        axes[i].set_title(f'Histogram + KDE: {var}')
    else:
        axes[i].text(0.5, 0.5, f'{var} not found', ha='center', va='center')
        axes[i].set_title(f'Error: {var}')
        axes[i].axis('off')

for j in range(i + 1, len(axes)): # Hide empty axles
    axes[j].axis('off')

plt.tight_layout()
plt.show()

"""As observed, most of the variables are skewed, with some even exhibiting multimodal or bimodal distributions. In addition, they contain outliers. *profundizar por variable

#### QQ Plots
"""

import scipy.stats as stats
import matplotlib.pyplot as plt
import pandas as pd

# Lista de variables a probar
plt.figure(figsize=(15, 10))

for i, var in enumerate(all_variables):
    plt.subplot(4, 5, i+1)

    # Ensure data is in a standard numeric format and remove NaNs
    data_to_plot = df[var].dropna()

    # Convert nullable integer types to standard int64
    if pd.api.types.is_integer_dtype(data_to_plot):
        data_to_plot = data_to_plot.astype('int64')

    stats.probplot(data_to_plot, dist="norm", plot=plt)
    plt.title(f'QQ Plot: {var}')

plt.tight_layout()
plt.show()

"""**Observations**

It is possible to observe the following conditions:
* Normal variables (almost): electricity mix, electricity sales (by sector)
* Anormal variables: real GDP, policy uncertainty, temperature variables and ev circulating stock.

All of the have some atypical points but that is an usual behavior in qq plots

#### Correlagrams
"""

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
for var in all_variables:
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    plot_acf(df[var].dropna(), ax=plt.gca(), lags=30)
    plt.title(f'ACF - {var}')

    plt.subplot(1, 2, 2)
    plot_pacf(df[var].dropna(), ax=plt.gca(), lags=30)
    plt.title(f'PACF - {var}')

    plt.tight_layout()
    plt.show()

"""**Observations**

The most of the variables show no stationary patterns. Based on the dynamics shown in the correlograms, they can be classified as follows:
1. Tendency: renewable energy, real gdp, policy uncertainty index, electricitty sales transportation and ev stock weighted
2. Cicles: fossil fuels, mixed energy, heating degree days, coolings degree days, electricity sales (residential, commercial, industrial) and electricity end use.

It indicates, in Data Preparation, it is possible to apply some non-functional trnasformations to smooth those behaviors.

### Bivariate Analysis

#### Scatter Plots
"""

n = len(independent_variables)
cols = 3
rows = (n // cols) + int(n % cols > 0)

plt.figure(figsize=(20, rows * 4))

for i, var in enumerate(independent_variables, 1):
    plt.subplot(rows, cols, i)
    plt.scatter(df[var], df[dependent_variable], alpha=0.6)
    plt.title(f"{var} vs. {dependent_variable}")
    plt.xlabel(var)
    plt.ylabel(dependent_variable)

plt.tight_layout()
plt.show()

"""**Observation**

1. Enegy mix: Fossil fuels and mixed energy have a clear and positive correlation between electricity use. Renewable energy does not express an strong correlation.
2. Economic Variables: These variables have a limited explicative power by themselves.
3. Temperature: These variables have a lineal relation with electricity use but their behavior is anormal due to when one of them is in the highest usage, the other one is in zero, reason why in the beggining of both plots the electricity changes continuosly.
4. Vehicles: It is not clear the existence of a real relation between ev circulating stock and electricity use.
5. Electricity Sales: Electricity sales by sector show a relationship with electricity end use, which is expected given that they are connected variables; however, they are not strongly correlated.

## Contextual exploration: renewable energy and EV Stock
"""

# Energy mix variables
mix_cols = ['fossil fuels', 'renewable energy', 'mixed energy']

# Calculation of total monthly and share of each source
df['total_generation'] = df[mix_cols].sum(axis=1)
df_share = df[mix_cols].div(df['total_generation'], axis=0)

df_share['date'] = df['date']

df_share.set_index('date')[mix_cols].plot(kind='line', figsize=(8, 4))

plt.title('Energy Mix Share Over Time')
plt.ylabel('Proportion of Total Generation')
plt.xlabel('Date')
plt.legend(title='Energy Source', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

fig, ax = plt.subplots(figsize=(8, 5))

# Graph: EV Stock vs Renewable Energy
ax.scatter(df['total_ev_stock'], df['renewable energy'], alpha=0.7, color='green')
ax.set_title('Total EV Stock vs Renewable Energy')
ax.set_xlabel('EV Stock')
ax.set_ylabel('Renewable Energy (MWh)')
ax.grid(True)

plt.tight_layout()
plt.show()

"""#3. Data Preparation
The purpose of Data Preparation phase is transform and modify the variables as much as they need to increase the accuracy and factibility of the forecasting model.

**Data Understanding conclusions**

* There are not null values, what is ideal in modeling.
* Good definition of data type, facilitating statistical and visual analysis.

**Separation**

  Train: January 2011 - December 2022

  Test: From January, 2023 - January 2025
"""

# Complete list of needed variables
all_variables = independent_variables + [dependent_variable]

# New DataFrame base
df_base = df[all_variables + ['date']].copy()

# Lag t-1 to all independent variables
for var in independent_variables:
    df_base[var] = df_base[var].shift(1)
df_base.dropna(inplace=True) #eliminar nulls

# Verify structure and nulls
print(df_base.info())
print()
print(df_base.isnull().sum())

"""### Seasons variable"""

def get_season(month):
    if month in [12, 1, 2]:
        return 'winter'
    elif month in [3, 4, 5]:
        return 'spring'
    elif month in [6, 7, 8]:
        return 'summer'
    else:
        return 'fall'

# New variable "Seasons"- Category
df_base['season'] = df_base['date'].dt.month.map(get_season)

# Verification
print(df_base[['date', 'season']].head())

# Create Dummy variables
df_base = pd.get_dummies(df_base, columns=['season'], drop_first=True)
print(df_base.tail())

# Identify bool columns
bool_cols = df_base.select_dtypes(include='bool').columns

# Conversion to Int (0 and 1)
df_base[bool_cols] = df_base[bool_cols].astype(int)

df_base.to_csv('df_base .csv', index=False)

print(df['electricity_end_use'].mean())

"""#4. Modeling

##First Set

### Iteration 1 (Diff + Mov Avg)

**Functional Non-Transformation**

To preserve the nature of the variables, the transformations will be applied based on the correlogram results. Specifically: for variables showing a trend, differencing will be used; for variables exhibiting cyclicality, moving averages will be applied. These techniques help smooth the behavior of the variables, sacrificing some observations in order to allow them to operate based on their own prior values and get stationaries variables.

*Differentiation:* It consist in resting the value immediately before to the actual.

*Mobile Averages:*
"""

df_trans = df_base.copy()
#df_trans.to_csv("antestrans.csv", index=False)

diff_variables = ['renewable energy', 'real_gdp', 'policy_uncertainty_index', 'total_ev_stock', 'electricity_sales_transportation']
med3_variables = ['fossil fuels', 'mixed energy', 'electricity_sales_residential']
med6_variables = ['heating_degree-days', 'cooling_degree-days','electricity_sales_commercial', 'electricity_sales_industrial']

for var in diff_variables:
    df_trans[f'{var}_diff'] = df_trans[var].diff()

for var in med3_variables:
    df_trans[f'{var}_ma3'] = df_trans[var].rolling(window=3).mean()

for var in med6_variables:
    df_trans[f'{var}_ma6'] = df_trans[var].rolling(window=6).mean()

# To delete original variables in case there are new to replace them
df_trans.drop(columns=[col for col in independent_variables if col in df_trans.columns], inplace=True)

df_trans.dropna(inplace=True)
df_trans.columns
df_trans.tail()

#df_trans.to_csv("outputf.csv", index=False)
print(df_trans.dtypes)

inddf_trans = df_trans[['date','season_spring', 'season_summer', 'season_winter',
       'renewable energy_diff', 'real_gdp_diff',
       'policy_uncertainty_index_diff', 'total_ev_stock_diff',
       'electricity_sales_transportation_diff', 'fossil fuels_ma3',
       'mixed energy_ma3', 'electricity_sales_residential_ma3', 'heating_degree-days_ma6',
       'cooling_degree-days_ma6', 'electricity_sales_commercial_ma6',
       'electricity_sales_industrial_ma6']]
depdf_trans = df_trans[['electricity_end_use']]

df_transm = pd.concat([inddf_trans, depdf_trans], axis=1)
df_transm.set_index('date', inplace=True)
print(df_transm.dtypes)

ind_vars_list1 = df_transm.columns.difference(depdf_trans.columns).tolist()
dep_var_list1 = depdf_trans.columns.tolist()

df_train1 = df_transm.loc[:'2022-12-31'].dropna()
df_test1 = df_transm.loc['2023-01-01':]

df_transm.to_csv('df_transm.csv', index=False)

x = df_train1[ind_vars_list1]
y = df_train1[dep_var_list1]

print("Variables independientes:", x.columns.tolist())
print("Variable dependiente:", dep_var_list1)

"""**Ridge**"""

from sklearn.linear_model import RidgeCV
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# 1. Definir alphas a probar
alphas = np.logspace(-3, 3, 50)

# 2. Ridge con validación cruzada
ridge_cv = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv.fit(x, y)

# 3. Resultados
print("Alpha óptimo:", ridge_cv.alpha_)
print("R² (entrenamiento cruzado):", ridge_cv.score(x, y))
y_pred = ridge_cv.predict(x)
rmse = np.sqrt(mean_squared_error(y, y_pred))
print("RMSE (entrenamiento):", rmse)

from sklearn.inspection import permutation_importance

result = permutation_importance(ridge_cv, x, y, n_repeats=30, random_state=0)
importance = pd.Series(result.importances_mean, index=x.columns).sort_values(ascending=False)
importance_rel = (importance / importance.sum()) * 100
print(importance_rel)

"""###Iteration 2 - (1 + Z Score)

**Z normalization**
"""

from sklearn.preprocessing import StandardScaler
# Seleccionar columnas numéricas que quieras estandarizar
cols_to_scale = df_trans.select_dtypes(include=[np.number]).columns
cols_to_scale = cols_to_scale.drop('electricity_end_use')

# Aplicar Z-score
scaler = StandardScaler()
df_scaled = df_trans.copy()
df_scaled[cols_to_scale] = scaler.fit_transform(df_trans[cols_to_scale])

inddf_scd = df_scaled[['date','season_spring', 'season_summer', 'season_winter',
       'renewable energy_diff', 'real_gdp_diff',
       'policy_uncertainty_index_diff', 'total_ev_stock_diff',
       'electricity_sales_transportation_diff', 'fossil fuels_ma3',
       'mixed energy_ma3', 'electricity_sales_residential_ma3', 'heating_degree-days_ma6',
       'cooling_degree-days_ma6', 'electricity_sales_commercial_ma6',
       'electricity_sales_industrial_ma6']]
depdf_scd = df_scaled[['electricity_end_use']]

df_scaledm = pd.concat([inddf_scd, depdf_scd], axis=1)
df_scaledm.set_index('date', inplace=True)

df_train2 = df_scaledm.loc[:'2022-12-31'].dropna()
df_test2 = df_scaledm.loc['2023-01-01':]

df_scaledm.to_csv('df_2.csv', index=False)

ind_vars_list2 = df_train2.columns.difference(depdf_scd.columns).tolist()
dep_var_list2 = depdf_scd.columns.tolist()

x2 = df_train2[ind_vars_list2]
y2 = df_train2[dep_var_list2]
x2.to_csv('x2.csv', index=False)
y2.to_csv('y2.csv', index=False)

print("Variables independientes:", x2.columns.tolist())
print("Variable dependiente:", dep_var_list2)

"""**Ridge**"""

from sklearn.linear_model import RidgeCV
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# 1. Definir alphas a probar
alphas = np.logspace(-3, 3, 50)

# 2. Ridge con validación cruzada
ridge_cv2 = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv2.fit(x2, y2)

# 3. Resultados
print("Alpha óptimo:", ridge_cv2.alpha_)
print("R² (entrenamiento cruzado):", ridge_cv2.score(x2, y2))
y2_pred = ridge_cv2.predict(x2)
rmse = np.sqrt(mean_squared_error(y2, y2_pred))
print("RMSE (entrenamiento):", rmse)

result2 = permutation_importance(ridge_cv2, x2, y2, n_repeats=30, random_state=0)
importance2 = pd.Series(result2.importances_mean, index=x2.columns).sort_values(ascending=False)
importance_rel2 = (importance2 / importance2.sum()) * 100
print(importance_rel2)

print(ridge_cv2.feature_names_in_)

"""
###Iteration 3 (log-log)"""

# Copiar base de datos
df_logs = df_base.copy()

# Seleccionar solo columnas numéricas, excepto 'date'
numeric_cols = df_logs.select_dtypes(include=np.number).columns.tolist()
if 'date' in numeric_cols:
    numeric_cols.remove('date')

# Variables que no deben ser transformadas con log
excluir_log = ['season_spring', 'season_summer', 'season_winter']
numeric_cols = [col for col in numeric_cols if col not in excluir_log]

# Aplicar transformación log y eliminar columnas originales
for var in numeric_cols:
    df_logs[f'log_{var}'] = np.log(df_logs[var] + 1)
    df_logs.drop(columns=[var], inplace=True)

df_logs.dropna(inplace=True)
print(df_logs.columns)

df_train3 = df_logs.loc[:'2022-12-31'].dropna()
df_test3 = df_logs.loc['2023-01-01':]

df_logs.to_csv('df_train3.csv', index=False)

dep_var_list3 = ['log_electricity_end_use']
ind_vars_list3 = [col for col in df_logs.columns if col not in dep_var_list3 and col != 'date']

x3 = df_train3[ind_vars_list3]
y3 = df_train3[dep_var_list3]

print("Variables independientes:", x3.columns.tolist())
print("Variable dependiente:", dep_var_list3)

"""**Ridge**"""

alphas = np.logspace(-3, 3, 50)

#Ridge + cross-validation
ridge_cv3 = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv3.fit(x3, y3)

# Results
print("Alpha óptimo:", ridge_cv3.alpha_)
print("R² (entrenamiento cruzado):", ridge_cv3.score(x3, y3))
y3_pred = ridge_cv3.predict(x3)
rmse = np.sqrt(mean_squared_error(y3, y3_pred))
print("RMSE (entrenamiento):", rmse)

# RMSE log to real scale:
rmse_real = np.exp(0.02452672333724416) - 1  # Aprox. %
print("RMSE en escala real:", rmse_real)

"""### Iteration 4 - (lin-log)"""

df_linlog = df_base.copy()

# Dummies are not transformed
dummies = ['season_spring', 'season_summer', 'season_winter']

# Selección de solo columnas numéricas, excluyendo 'date', la variable dependiente y las dummies
numeric_cols = df_linlog.select_dtypes(include=np.number).columns.tolist()
numeric_cols = [col for col in numeric_cols
                if col not in dummies
                and col != 'electricity_end_use'
                and col != 'date']


for var in numeric_cols:
    df_linlog[f'log_{var}'] = np.log(df_linlog[var] + 1)
    df_linlog.drop(columns=[var], inplace=True)

df_linlog.dropna(inplace=True)

print(df_linlog.columns)

df_train4 = df_linlog.loc[:'2022-12-31'].dropna()
df_test4 = df_linlog.loc['2023-01-01':]

df_linlog.to_csv('df_train4s.csv', index=False)

dep_var_list4 = ['electricity_end_use']
ind_vars_list4 = [col for col in df_linlog.columns if col not in dep_var_list4 and col != 'date']

x4 = df_train4[ind_vars_list4]
y4 = df_train4[dep_var_list4]

print("Variables independientes:", x4.columns.tolist())
print("Variable dependiente:", dep_var_list4)

"""**Ridge**"""

alphas = np.logspace(-3, 3, 50)

ridge_cv4 = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv4.fit(x4, y4)

print("Alpha óptimo:", ridge_cv4.alpha_)
print("R² (entrenamiento cruzado):", ridge_cv4.score(x4, y4))
y4_pred = ridge_cv4.predict(x4)
rmse = np.sqrt(mean_squared_error(y4, y4_pred))
print("RMSE (entrenamiento):", rmse)

result4 = permutation_importance(ridge_cv4, x4, y4, n_repeats=30, random_state=0)
importance4 = pd.Series(result4.importances_mean, index=x4.columns).sort_values(ascending=False)
importance_rel4 = (importance4 / importance4.sum()) * 100
print(importance_rel4)

"""###Iteration 5 - (log-lin)"""

df_loglin = df_base.copy()

if 'electricity_end_use' in df_loglin.columns:
    df_loglin['log_electricity_end_use'] = np.log(df_loglin['electricity_end_use'] + 1)
    df_loglin.drop(columns=['electricity_end_use'], inplace=True)

df_loglin.dropna(inplace=True)
print(df_loglin.columns)

df_train5 = df_loglin.loc[:'2022-12-31'].dropna()
df_test5 = df_loglin.loc['2023-01-01':]

df_loglin.to_csv('df_train5.csv', index=False)

dep_var_list5 = ['log_electricity_end_use']
ind_vars_list5 = [col for col in df_loglin.columns if col not in dep_var_list5 and col != 'date']

x5 = df_train5[ind_vars_list5]
y5 = df_train5[dep_var_list5]

print("Variables independientes:", x5.columns.tolist())
print("Variable dependiente:", dep_var_list5)

"""**Ridge**"""

alphas = np.logspace(-3, 3, 50)

ridge_cv5 = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv5.fit(x5, y5)

print("Alpha óptimo:", ridge_cv5.alpha_)
print("R² (entrenamiento cruzado):", ridge_cv5.score(x5, y5))
y5_pred = ridge_cv5.predict(x5)
rmse = np.sqrt(mean_squared_error(y5, y5_pred))
print("RMSE (entrenamiento):", rmse)

rmse_real5 = np.exp(0.009099799459830336) - 1
print("RMSE5 en escala real:", rmse_real5)

"""## Second Set

### Iteration 6 (dff + mov avg)
"""

# Selection of the most important variables
ind_vars_list6 = [
    'season_summer',
    'season_spring',
    'electricity_sales_residential_ma3',
    'cooling_degree-days_ma6',
    'fossil fuels_ma3',
    'season_winter'
]

# Dependent variable
dep_var_list6 = ['electricity_end_use']

x6 = df_train1[ind_vars_list6]
y6 = df_train1[dep_var_list6]

print("Variables independientes:", x6.columns.tolist())
print("Variable dependiente:", dep_var_list6)

alphas = np.logspace(-3, 3, 50)

ridge_cv6 = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv6.fit(x6, y6)

print("Alpha óptimo:", ridge_cv6.alpha_)
print("R² (train):", ridge_cv6.score(x6, y6))
y6_pred = ridge_cv6.predict(x6)
rmse6 = np.sqrt(mean_squared_error(y6, y6_pred))
print("RMSE (train):", rmse6)

result6 = permutation_importance(ridge_cv6, x6, y6, n_repeats=30, random_state=0)
importance6 = pd.Series(result6.importances_mean, index=x6.columns).sort_values(ascending=False)
importance_rel6 = (importance6 / importance6.sum()) * 100
print(importance_rel6)

"""### Iteration 7 (dff + mov avg)"""

# Variables selection
ind_vars_list7 = [
    'season_summer',
    'season_spring',
    'electricity_sales_residential_ma3',
    'cooling_degree-days_ma6',
    'fossil fuels_ma3',
    'season_winter',
    'total_ev_stock_diff'
]

dep_var_list7 = ['electricity_end_use']

x7 = df_train1[ind_vars_list7]
y7 = df_train1[dep_var_list7]

print("Variables independientes:", x7.columns.tolist())
print("Variable dependiente:", dep_var_list7)

alphas = np.logspace(-3, 3, 50)

ridge_cv7 = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv7.fit(x7, y7)

print("Alpha óptimo:", ridge_cv7.alpha_)
print("R² (train):", ridge_cv7.score(x7, y7))
y7_pred = ridge_cv7.predict(x7)
rmse7 = np.sqrt(mean_squared_error(y7, y7_pred))
print("RMSE (train):", rmse7)

result7 = permutation_importance(ridge_cv7, x7, y7, n_repeats=30, random_state=0)
importance7 = pd.Series(result7.importances_mean, index=x7.columns).sort_values(ascending=False)
importance_rel7 = (importance7 / importance7.sum()) * 100
print(importance_rel7)

"""### Iteration 8 (dff + mov avg)"""

# Variable selection
ind_vars_list8 = [
    'season_summer',
    'season_spring',
    'electricity_sales_residential_ma3',
    'cooling_degree-days_ma6',
    'fossil fuels_ma3',
    'season_winter',
    'total_ev_stock_diff',
    'policy_uncertainty_index_diff'
]

dep_var_list8 = ['electricity_end_use']

x8 = df_train1[ind_vars_list8]
y8 = df_train1[dep_var_list8]

print("Variables independientes:", x8.columns.tolist())
print("Variable dependiente:", dep_var_list8)

alphas = np.logspace(-3, 3, 50)

ridge_cv8 = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv8.fit(x8, y8)

print("Alpha óptimo:", ridge_cv8.alpha_)
print("R² (train):", ridge_cv8.score(x8, y8))
y8_pred = ridge_cv8.predict(x8)
rmse8 = np.sqrt(mean_squared_error(y8, y8_pred))
print("RMSE (train):", rmse8)

result8 = permutation_importance(ridge_cv8, x8, y8, n_repeats=30, random_state=0)
importance8 = pd.Series(result8.importances_mean, index=x8.columns).sort_values(ascending=False)
importance_rel8 = (importance8 / importance8.sum()) * 100
print(importance_rel8)

"""### Iteration 9 (dff + mov avg)"""

# Variables selection
ind_vars_list9 = [
    'electricity_sales_residential_ma3',
    'cooling_degree-days_ma6',
    'fossil fuels_ma3',
    'season_summer'
]

dep_var_list9 = ['electricity_end_use']

x9 = df_train1[ind_vars_list9]
y9 = df_train1[dep_var_list9]

print("Variables independientes:", x9.columns.tolist())
print("Variable dependiente:", dep_var_list9)

alphas = np.logspace(-3, 3, 50)

ridge_cv9 = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv9.fit(x9, y9)

print("Alpha óptimo:", ridge_cv9.alpha_)
print("R² (train):", ridge_cv9.score(x9, y9))
y9_pred = ridge_cv9.predict(x9)
rmse9 = np.sqrt(mean_squared_error(y9, y9_pred))
print("RMSE (train):", rmse9)

result9 = permutation_importance(ridge_cv9, x9, y9, n_repeats=30, random_state=0)
importance9 = pd.Series(result9.importances_mean, index=x9.columns).sort_values(ascending=False)
importance_rel9 = (importance9 / importance9.sum()) * 100
print(importance_rel9)

"""
### Iteration 10 (dff + mov avg)"""

# Variable selection
ind_vars_list10 = [
    'season_spring',
    'electricity_sales_residential_ma3',
    'cooling_degree-days_ma6',
    'fossil fuels_ma3',
    'season_winter',
    'total_ev_stock_diff',
    'policy_uncertainty_index_diff',
    'season_summer',
    'real_gdp_diff'
]

dep_var_list10 = ['electricity_end_use']

x10 = df_train1[ind_vars_list10]
y10 = df_train1[dep_var_list10]

print("Variables independientes:", x10.columns.tolist())
print("Variable dependiente:", dep_var_list10)

alphas = np.logspace(-3, 3, 50)

ridge_cv10 = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv10.fit(x10, y10)

print("Alpha óptimo:", ridge_cv10.alpha_)
print("R² (train):", ridge_cv10.score(x10, y10))
y10_pred = ridge_cv10.predict(x10)
rmse10 = np.sqrt(mean_squared_error(y10, y10_pred))
print("RMSE (train):", rmse10)

result10 = permutation_importance(ridge_cv10, x10, y10, n_repeats=30, random_state=0)
importance10 = pd.Series(result10.importances_mean, index=x10.columns).sort_values(ascending=False)
importance_rel10 = (importance10 / importance10.sum()) * 100
print(importance_rel10)

"""### Iteration 11 (lin-log)"""

# Variables selection
ind_vars_list11 = [
    'log_heating_degree-days',
    'season_summer',
    'log_cooling_degree-days',
    'season_winter',
    'log_total_ev_stock'
]

dep_var_list11 = ['electricity_end_use']

x11 = df_train4[ind_vars_list11]
y11 = df_train4[dep_var_list11]

print("Variables independientes:", x11.columns.tolist())
print("Variable dependiente:", dep_var_list11)

alphas = np.logspace(-3, 3, 50)

ridge_cv11 = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv11.fit(x11, y11)

print("Alpha óptimo:", ridge_cv11.alpha_)
print("R² (train):", ridge_cv11.score(x11, y11))
y11_pred = ridge_cv11.predict(x11)
rmse11 = np.sqrt(mean_squared_error(y11, y11_pred))
print("RMSE (train):", rmse11)

result11 = permutation_importance(ridge_cv11, x11, y11, n_repeats=30, random_state=0)
importance11 = pd.Series(result11.importances_mean, index=x11.columns).sort_values(ascending=False)
importance_rel11 = (importance11 / importance11.sum()) * 100
print(importance_rel11)

"""## Third Set

### Iteration 12 (1+ Z score)
"""

# Variables selection - Based on iteration 8
ind_vars_list12 = [
    'season_summer',
    'season_spring',
    'electricity_sales_residential_ma3',
    'cooling_degree-days_ma6',
    'fossil fuels_ma3',
    'season_winter',
    'total_ev_stock_diff',
    'policy_uncertainty_index_diff'
]

dep_var_list12 = ['electricity_end_use']

x12 = df_train2[ind_vars_list12]
y12 = df_train2[dep_var_list12]

print("Variables independientes:", x12.columns.tolist())
print("Variable dependiente:", dep_var_list12)

alphas = np.logspace(-3, 3, 50)

ridge_cv12 = RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')
ridge_cv12.fit(x12, y12)

print("Alpha óptimo:", ridge_cv12.alpha_)
print("R² (train):", ridge_cv12.score(x12, y12))
y12_pred = ridge_cv12.predict(x12)
rmse12 = np.sqrt(mean_squared_error(y12, y12_pred))
print("RMSE (train):", rmse12)

result12 = permutation_importance(ridge_cv12, x12, y12, n_repeats=30, random_state=0)
importance12 = pd.Series(result12.importances_mean, index=x12.columns).sort_values(ascending=False)
importance_rel12 = (importance12 / importance12.sum()) * 100
print(importance_rel12)

df_train2.to_csv('dftrain2.csv', index=False)

#Download
df_train_results = pd.DataFrame({
    'electricity_end_use_actual': y12.values.flatten(),
    'electricity_end_use_predicted': y12_pred.flatten()
}, index=x12.index)

df_train_results.to_csv('ridge_iteration12_train.csv', index_label='datetime')
print("Archivo ridge_iteration12_train.csv guardado.")

"""# Evaluation

##Fist Set 1-5

### Iteration 1 (+tests)
"""

# Dependent and independent variables of test sample
x_test1 = df_test1[ind_vars_list1]
y_test1 = df_test1[dep_var_list1]

# Predictions
y_pred_test1 = ridge_cv.predict(x_test1)

# Evaluation of the model (test)
from sklearn.metrics import mean_squared_error, r2_score

rmse_test1 = np.sqrt(mean_squared_error(y_test1, y_pred_test1))
r2_test1 = r2_score(y_test1, y_pred_test1)

# Results
print("📘  Evaluation in testing (Model 1):")
print("R² (test):", r2_test1)
print("RMSE (test):", rmse_test1)

"""**Pruebas**
* Normalidad residual
* Multicolinearity
* Autocorrelatión
* Homoscedastidad
"""

#Normality
import scipy.stats as stats
import seaborn as sns
import statsmodels.api as sm

# Residual
y_series = y.squeeze()
residuals = y_series - y_pred

# Shapiro-Wilk test
shapiro_test = stats.shapiro(residuals)
print("Shapiro-Wilk test:")
print("W =", shapiro_test.statistic, "| p-value =", shapiro_test.pvalue)

#Multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor
import numpy as np

vif_data = pd.DataFrame()
vif_data["Variable"] = x.columns

# Convert nullable integer columns to int64 for VIF calculation
for col in x.columns:
    if pd.api.types.is_integer_dtype(x[col].dtype):
        x[col] = x[col].astype(np.int64)

# Calculate VIF
vif_data["VIF"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]
print(vif_data)

# Autocorrelation of residues
from statsmodels.stats.stattools import durbin_watson

dw_stat = durbin_watson(residuals)
print("Durbin-Watson:", dw_stat)

#Homocedasticity
import statsmodels.api as sm

# Add a constant to the independent variables
x_with_constant = sm.add_constant(x)

bp_test = sm.stats.diagnostic.het_breuschpagan(residuals, x_with_constant)
labels = ['LM Stat', 'LM p-val', 'F Stat', 'F p-val']
print("Breusch-Pagan test:")
print(dict(zip(labels, bp_test)))

"""### Iteration 2 (+test)"""

# Test
x_test2 = df_test2[ind_vars_list2]
y_test2 = df_test2[dep_var_list2]

# Predictions
y_pred_test2 = ridge_cv2.predict(x_test2)

from sklearn.metrics import mean_squared_error, r2_score

rmse_test2 = np.sqrt(mean_squared_error(y_test2, y_pred_test2))
r2_test2 = r2_score(y_test2, y_pred_test2)

# Results
print("📘  Evaluation in testing (Model 2):")
print("R² (test):", r2_test2)
print("RMSE (test):", rmse_test2)

#Normality
import scipy.stats as stats
import seaborn as sns
import statsmodels.api as sm

# Residues
y2_series = y2.squeeze()
residuals2 = y2_series - y2_pred

# Shapiro-Wilk test
shapiro_test = stats.shapiro(residuals2)
print("Shapiro-Wilk test:")
print("W =", shapiro_test.statistic, "| p-value =", shapiro_test.pvalue)

# Multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Numeric columns
x2_numeric = x2.select_dtypes(include=["number"]).astype(float)

# Replace NaN
x2_numeric = x2_numeric.replace([np.inf, -np.inf], np.nan)
x2_numeric = x2_numeric.dropna()

# Calculate VIF
vif2_data = pd.DataFrame()
vif2_data["Variable"] = x2_numeric.columns
vif2_data["VIF"] = [variance_inflation_factor(x2_numeric.values, i)
                    for i in range(x2_numeric.shape[1])]

print(vif2_data)

# Autocorrelation of residues
from statsmodels.stats.stattools import durbin_watson

dw_stat = durbin_watson(residuals2)
print("Durbin-Watson:", dw_stat)

#Homocedasticity
import statsmodels.api as sm

x2_bp = sm.add_constant(x2)
bp_test = sm.stats.diagnostic.het_breuschpagan(residuals2, x2_bp)

labels = ['LM Stat', 'LM p-val', 'F Stat', 'F p-val']
print(dict(zip(labels, bp_test)))

# Condition Index Test
from numpy.linalg import svd

# 1. Matrix X without scaling
X = x2.astype(float).values

# 2. Compute SVD of X
U, s, Vt = svd(X, full_matrices=False)

# 3. Compute eigenvalues
eigenvalues = s**2 / (X.shape[0] - 1)

# 4. Compute Condition Index
condition_index = np.sqrt(eigenvalues.max() / eigenvalues)

# 5. Display results
condition_df = pd.DataFrame({
    "Eigenvalue": eigenvalues,
    "Condition Index": condition_index
})

print("\nCondition Index by dimension:")
print(condition_df)

# Or just the maximum index:
max_condition_index = condition_index.max()
print(f"\n🔎 Maximum Condition Index: {max_condition_index}")
print(x2.dtypes)

"""### Iteration 3"""

# Test
x_test3 = df_test3[ind_vars_list3]
y_test3 = df_test3[dep_var_list3]

# Predictions
y3_pred_test = ridge_cv3.predict(x_test3)

from sklearn.metrics import mean_squared_error, r2_score
rmse_test3 = np.sqrt(mean_squared_error(y_test3, y3_pred_test))
r2_test3 = r2_score(y_test3, y3_pred_test)

# Results
print("📘  Evaluation in testing (Model 3):")
print("R² (test):", r2_test3)
print("RMSE (test):", rmse_test3)

"""### Iteration 4 (+tests)"""

#Test
x_test4 = df_test4[ind_vars_list4]
y_test4 = df_test4[dep_var_list4]

# Predictions
y4_pred_test = ridge_cv4.predict(x_test4)

from sklearn.metrics import mean_squared_error, r2_score
rmse_test4 = np.sqrt(mean_squared_error(y_test4, y4_pred_test))
r2_test4 = r2_score(y_test4, y4_pred_test)

# Results
print("📘  Evaluation in testing (Model 4):")
print("R² (test):", r2_test4)
print("RMSE (test):", rmse_test4)

print("⚙️ Evaluación en entrenamiento:")
print("R² (train):", ridge_cv4.score(x4, y4))
print("RMSE (train):", rmse)

"""**Pruebas**
* Normalidad residual
* Multicolinearity
* Autocorrelatión
* Homoscedastidad
"""

#Normality
import scipy.stats as stats
import seaborn as sns
import statsmodels.api as sm

# Residues
y4_series = y4.squeeze()
residuals4 = y4_series - y4_pred

# Shapiro-Wilk test
shapiro_test = stats.shapiro(residuals)
print("Shapiro-Wilk test:")
print("W =", shapiro_test.statistic, "| p-value =", shapiro_test.pvalue)

# Multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

x4_numeric = x4.select_dtypes(include=["number"]).astype(float)
x4_numeric = x4_numeric.replace([np.inf, -np.inf], np.nan)
x4_numeric = x4_numeric.dropna()

# Calculate VIF
vif4_data = pd.DataFrame()
vif4_data["Variable"] = x4_numeric.columns
vif4_data["VIF"] = [variance_inflation_factor(x4_numeric.values, i)
                    for i in range(x4_numeric.shape[1])]

print(vif4_data)

# Autocorrelation of residues
from statsmodels.stats.stattools import durbin_watson

dw_stat = durbin_watson(residuals4)
print("Durbin-Watson:", dw_stat)

#Homocedasticity
import statsmodels.api as sm

x4_bp = sm.add_constant(x4)
bp_test = sm.stats.diagnostic.het_breuschpagan(residuals4, x4_bp)

labels = ['LM Stat', 'LM p-val', 'F Stat', 'F p-val']
print(dict(zip(labels, bp_test)))

"""### Iteration 5"""

# Test
x_test5 = df_test5[ind_vars_list5]
y_test5 = df_test5[dep_var_list5]

# Predictions
y5_pred_test = ridge_cv5.predict(x_test5)

from sklearn.metrics import mean_squared_error, r2_score
rmse_test5 = np.sqrt(mean_squared_error(y_test5, y5_pred_test))
r2_test5 = r2_score(y_test5, y5_pred_test)

# Resultados
print("📘 Evaluation in testing (Model 5):")
print("R² (test):", r2_test5)
print("RMSE (test):", rmse_test5)

"""## Second Set 6-10

### Iteration 6 (+tests)
"""

# Test
x_test6 = df_test1[ind_vars_list6]
y_test6 = df_test1[dep_var_list6]

# Prediction
y_test6_pred = ridge_cv6.predict(x_test6)

r2_test6 = r2_score(y_test6, y_test6_pred)
rmse_test6 = np.sqrt(mean_squared_error(y_test6, y_test6_pred))

print("🔍 Evaluación en test:")
print("R² (test):", r2_test6)
print("RMSE (test):", rmse_test6)

"""**Pruebas**
* Normalidad residual
* Multicolinearity
* Autocorrelatión
* Homoscedastidad
"""

#Normality
import scipy.stats as stats
import seaborn as sns
import statsmodels.api as sm

# Residues
y6_series = y6.squeeze()
residuals6 = y6_series - y6_pred

# Shapiro-Wilk test
shapiro_test = stats.shapiro(residuals6)
print("Shapiro-Wilk test:")
print("W =", shapiro_test.statistic, "| p-value =", shapiro_test.pvalue)

# Multicolinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

x6_numeric = x6.select_dtypes(include=["number"]).astype(float)
x6_numeric = x6_numeric.replace([np.inf, -np.inf], np.nan)
x6_numeric = x6_numeric.dropna()

# Calculate VIF
vif6_data = pd.DataFrame()
vif6_data["Variable"] = x6_numeric.columns
vif6_data["VIF"] = [variance_inflation_factor(x6_numeric.values, i)
                    for i in range(x6_numeric.shape[1])]

print(vif6_data)

# Autocorrelation de residues
from statsmodels.stats.stattools import durbin_watson

dw_stat = durbin_watson(residuals6)
print("Durbin-Watson:", dw_stat)

#Homocedasticity
import statsmodels.api as sm

x6_bp = sm.add_constant(x6)
bp_test = sm.stats.diagnostic.het_breuschpagan(residuals6, x6_bp)

labels = ['LM Stat', 'LM p-val', 'F Stat', 'F p-val']
print(dict(zip(labels, bp_test)))

"""### Iteration 7"""

# Test
x_test7 = df_test1[ind_vars_list7]
y_test7 = df_test1[dep_var_list7]

# Prediction
y_test7_pred = ridge_cv7.predict(x_test7)

# Evaluation in test
r2_test7 = r2_score(y_test7, y_test7_pred)
rmse_test7 = np.sqrt(mean_squared_error(y_test7, y_test7_pred))

print("🔍 Evaluación en test:")
print("R² (test):", r2_test7)
print("RMSE (test):", rmse_test7)

"""### Iteration 8 (+tests)"""

# Test
x_test8 = df_test1[ind_vars_list8]
y_test8 = df_test1[dep_var_list8]

# Prediction
y_test8_pred = ridge_cv8.predict(x_test8)

r2_test8 = r2_score(y_test8, y_test8_pred)
rmse_test8 = np.sqrt(mean_squared_error(y_test8, y_test8_pred))

print("🔍 Evaluación en test:")
print("R² (test):", r2_test8)
print("RMSE (test):", rmse_test8)

"""**Pruebas**
* Normalidad residual
* Multicolinearity
* Autocorrelatión
* Homoscedastidad
"""

#Normality
import scipy.stats as stats
import seaborn as sns
import statsmodels.api as sm

# Residues
y8_series = y8.squeeze()
residuals8 = y8_series - y8_pred

# Shapiro-Wilk test
shapiro_test = stats.shapiro(residuals8)
print("Shapiro-Wilk test:")
print("W =", shapiro_test.statistic, "| p-value =", shapiro_test.pvalue)

# Multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

x8_numeric = x8.select_dtypes(include=["number"]).astype(float)
x8_numeric = x8_numeric.replace([np.inf, -np.inf], np.nan)
x8_numeric = x8_numeric.dropna()

# Calculate VIF
vif8_data = pd.DataFrame()
vif8_data["Variable"] = x8_numeric.columns
vif8_data["VIF"] = [variance_inflation_factor(x8_numeric.values, i)
                    for i in range(x8_numeric.shape[1])]

print(vif8_data)

# Autocorrelation of residues
from statsmodels.stats.stattools import durbin_watson

dw_stat = durbin_watson(residuals8)
print("Durbin-Watson:", dw_stat)

#Homocedasticity
import statsmodels.api as sm

x8_bp = sm.add_constant(x8)  # Agrega columna de unos
bp_test = sm.stats.diagnostic.het_breuschpagan(residuals8, x8_bp)

labels = ['LM Stat', 'LM p-val', 'F Stat', 'F p-val']
print(dict(zip(labels, bp_test)))

numeric_df8 = df_train1[ind_vars_list8].select_dtypes(include=[np.number])# Correlation Matrix

correlation_matrix8 = numeric_df8.corr() # Calcular matriz de correlación
#mask = np.triu(np.ones_like(correlation_matrix, dtype=bool)) # Crear una máscara para la mitad superior

plt.figure(figsize=(10, 8))# Graficar la matriz de correlación con solo la mitad inferior
sns.heatmap(correlation_matrix8, annot=True, fmt=".2f", cmap='coolwarm', square=True, linewidths=.5, cbar_kws={"shrink": .8})
plt.title("Correlation Matrix between Numeric Variables Model 8")
plt.tight_layout()
plt.show()

# Condition Index Test
from numpy.linalg import svd

# 1. Matrix X without scaling
X = x8.astype(float).values  # if x8 is a DataFrame. If it's already an array, omit .values

# 2. Compute SVD of X
U, s, Vt = svd(X, full_matrices=False)

# 3. Compute eigenvalues
eigenvalues = s**2 / (X.shape[0] - 1)

# 4. Compute Condition Index
condition_index = np.sqrt(eigenvalues.max() / eigenvalues)

# 5. Display results
condition_df = pd.DataFrame({
    "Eigenvalue": eigenvalues,
    "Condition Index": condition_index
})

print("\nCondition Index by dimension:")
print(condition_df)

# Or just the maximum index:
max_condition_index = condition_index.max()
print(f"\n🔎 Maximum Condition Index: {max_condition_index}")
print(x8.dtypes)

"""### Iteration 9"""

# Variables para test
x_test9 = df_test1[ind_vars_list9]
y_test9 = df_test1[dep_var_list9]

# Predicción
y_test9_pred = ridge_cv9.predict(x_test9)

# Evaluación en test
r2_test9 = r2_score(y_test9, y_test9_pred)
rmse_test9 = np.sqrt(mean_squared_error(y_test9, y_test9_pred))

print("🔍 Evaluación en test:")
print("R² (test):", r2_test9)
print("RMSE (test):", rmse_test9)

"""### Iteration 10 (+tests)"""

# Test
x_test10 = df_test1[ind_vars_list10]
y_test10 = df_test1[dep_var_list10]

# Prediction
y_test10_pred = ridge_cv10.predict(x_test10)

r2_test10 = r2_score(y_test10, y_test10_pred)
rmse_test10 = np.sqrt(mean_squared_error(y_test10, y_test10_pred))

print("🔍 Evaluación en test:")
print("R² (test):", r2_test10)
print("RMSE (test):", rmse_test10)

"""**Pruebas**
* Normalidad residual
* Multicolinearity
* Autocorrelatión
* Homoscedastidad
"""

#Normality
import scipy.stats as stats
import seaborn as sns
import statsmodels.api as sm

# Residues
y10_series = y10.squeeze()
residuals10 = y10_series - y10_pred

# Shapiro-Wilk test
shapiro_test = stats.shapiro(residuals10)
print("Shapiro-Wilk test:")
print("W =", shapiro_test.statistic, "| p-value =", shapiro_test.pvalue)

# Multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

x10_numeric = x10.select_dtypes(include=["number"]).astype(float)
x10_numeric = x10_numeric.replace([np.inf, -np.inf], np.nan)
x10_numeric = x10_numeric.dropna()

# Calculate VIF
vif10_data = pd.DataFrame()
vif10_data["Variable"] = x10_numeric.columns
vif10_data["VIF"] = [variance_inflation_factor(x10_numeric.values, i)
                    for i in range(x10_numeric.shape[1])]

print(vif10_data)

# Autocorrelation of residues
from statsmodels.stats.stattools import durbin_watson

dw_stat = durbin_watson(residuals10)
print("Durbin-Watson:", dw_stat)

#Homoscedasticity
import statsmodels.api as sm

x10_bp = sm.add_constant(x10)
bp_test = sm.stats.diagnostic.het_breuschpagan(residuals10, x10_bp)

labels = ['LM Stat', 'LM p-val', 'F Stat', 'F p-val']
print(dict(zip(labels, bp_test)))

"""### Iteration 11 (+tests)"""

# Test
x_test11 = df_test4[ind_vars_list11]
y_test11 = df_test4[dep_var_list11]

# Prediction
y_test11_pred = ridge_cv11.predict(x_test11)

r2_test11 = r2_score(y_test11, y_test11_pred)
rmse_test11 = np.sqrt(mean_squared_error(y_test11, y_test11_pred))

print("🔍 Evaluación en test:")
print("R² (test):", r2_test11)
print("RMSE (test):", rmse_test11)

"""**Pruebas**
* Normalidad residual
* Multicolinearity
* Autocorrelatión
* Homoscedastidad
"""

#Normality
import scipy.stats as stats
import seaborn as sns
import statsmodels.api as sm

# Residues
y11_series = y11.squeeze()
residuals11 = y11_series - y11_pred

# Shapiro-Wilk test
shapiro_test = stats.shapiro(residuals11)
print("Shapiro-Wilk test:")
print("W =", shapiro_test.statistic, "| p-value =", shapiro_test.pvalue)

# Multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

x11_numeric = x11.select_dtypes(include=["number"]).astype(float)
x11_numeric = x11_numeric.replace([np.inf, -np.inf], np.nan)
x11_numeric = x11_numeric.dropna()

# Calculate VIF
vif11_data = pd.DataFrame()
vif11_data["Variable"] = x11_numeric.columns
vif11_data["VIF"] = [variance_inflation_factor(x11_numeric.values, i)
                    for i in range(x11_numeric.shape[1])]

print(vif11_data)

# Autocorrelation of residues
from statsmodels.stats.stattools import durbin_watson

dw_stat = durbin_watson(residuals11)
print("Durbin-Watson:", dw_stat)

#Homocedasticity
import statsmodels.api as sm

x11_bp = sm.add_constant(x11)
bp_test = sm.stats.diagnostic.het_breuschpagan(residuals11, x11_bp)

labels = ['LM Stat', 'LM p-val', 'F Stat', 'F p-val']
print(dict(zip(labels, bp_test)))

"""### Iteration 12 (+tests)"""

# Test
x_test12 = df_test2[ind_vars_list12]
y_test12 = df_test2[dep_var_list12]

# Predictions
y_pred_test12 = ridge_cv12.predict(x_test12)

from sklearn.metrics import mean_squared_error, r2_score

rmse_test12 = np.sqrt(mean_squared_error(y_test12, y_pred_test12))
r2_test12 = r2_score(y_test12, y_pred_test12)

# Results
print("📘  Evaluation in testing (Model 2):")
print("R² (test):", r2_test12)
print("RMSE (test):", rmse_test12)

x_test12

y_test12

#Normality
import scipy.stats as stats
import seaborn as sns
import statsmodels.api as sm

# Residues
y12_series = y12.squeeze()
residuals12 = y12_series - y12_pred

# Shapiro-Wilk test
shapiro_test = stats.shapiro(residuals12)
print("Shapiro-Wilk test:")
print("W =", shapiro_test.statistic, "| p-value =", shapiro_test.pvalue)

# Multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

x12_numeric = x12.select_dtypes(include=["number"]).astype(float)
x12_numeric = x12_numeric.replace([np.inf, -np.inf], np.nan)
x12_numeric = x12_numeric.dropna()

# Calculate VIF
vif12_data = pd.DataFrame()
vif12_data["Variable"] = x12_numeric.columns
vif12_data["VIF"] = [variance_inflation_factor(x12_numeric.values, i)
                    for i in range(x12_numeric.shape[1])]

print(vif12_data)

# Autocorrelation of residues
from statsmodels.stats.stattools import durbin_watson

dw_stat = durbin_watson(residuals12)
print("Durbin-Watson:", dw_stat)

#Homocedasticity
import statsmodels.api as sm

x12_bp = sm.add_constant(x12)
bp_test = sm.stats.diagnostic.het_breuschpagan(residuals12, x12_bp)

labels = ['LM Stat', 'LM p-val', 'F Stat', 'F p-val']
print(dict(zip(labels, bp_test)))

# Condition Index Test
from numpy.linalg import svd

# 1. Matrix X without scaling
X12 = x12.astype(float).values

# 2. Compute SVD of X
U, s, Vt = svd(X12, full_matrices=False)

# 3. Compute eigenvalues
eigenvalues = s**2 / (X12.shape[0] - 1)

# 4. Compute Condition Index
condition_index = np.sqrt(eigenvalues.max() / eigenvalues)

# 5. Display results
condition_df = pd.DataFrame({
    "Eigenvalue": eigenvalues,
    "Condition Index": condition_index
})

print("\nCondition Index by dimension:")
print(condition_df)

# Or just the maximum index:
max_condition_index = condition_index.max()
print(f"\n🔎 Maximum Condition Index: {max_condition_index}")
print(x12.dtypes)

#Download
df_test_results = pd.DataFrame({
    'electricity_end_use_actual': y_test12.values.flatten(),
    'electricity_end_use_predicted': y_pred_test12.flatten()
}, index=x_test12.index)

# CVS
df_test_results.to_csv('ridge_iteration12_test.csv', index_label='datetime')
print("Archivo ridge_iteration12_test.csv guardado.")

"""# Feedback model results"""

# REAL
# --------------------------------------------

# Real - TRAIN
real_train = df_train_results['electricity_end_use_actual']
# Predicted - TRAIN
pred_train = df_train_results['electricity_end_use_predicted']

# Real - TEST
real_test = df_test_results['electricity_end_use_actual']
# Predicted - TEST
pred_test = df_test_results['electricity_end_use_predicted']

# Plot
# --------------------------------------------

plt.figure(figsize=(14,6))

# Solid REAL line
# ------------------------
# Train
plt.plot(real_train.index, real_train.values,
         color='blue', linestyle='-', linewidth=2, label='Actual (Train)')

# Test
plt.plot(real_test.index, real_test.values,
         color='green', linestyle='-', linewidth=2, label='Actual (Test)')

# Dashed PREDICTED lines
# ------------------------
# Train
plt.plot(pred_train.index, pred_train.values,
         color='blue', linestyle='--', linewidth=2, label='Model (Train)')

# Test
plt.plot(pred_test.index, pred_test.values,
         color='green', linestyle='--', linewidth=2, label='Model (Test)')

plt.title('Actual Electricity vs. Model Predictions')
plt.xlabel('Date')
plt.ylabel('Electricity (MWh)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()